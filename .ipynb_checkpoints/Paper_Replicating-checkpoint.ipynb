{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd31029-3734-43d2-8043-9376a117c0e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we need updated APIs, so we need updates after torch 1.12+ and torchvision 0.13+\n",
    "# since the torch version is  already 2.2.2+ and the torchvision version is 0.17.2+ -- we need to assert with right version and this can help install the newer updates \n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[0]) >=1 and int(torch.__version__.split(\".\")[1]) >=12, \"torch version should be more than 1.12+\"                                                           \n",
    "    assert int(torch.__version__.split(\".\")[1]) >=13, \"torchvision version should be more than 1.12+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch.torchvision versions not as required, installing updated versions\")\n",
    "    !pip install -U torch torchvision torchaudio --extra-index-url https://dowload.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf5e3f-1307-4a61-a471-24da958109b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts import data_setup, engine, download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61c887-1ebc-466e-b45a-224cb17c4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms \n",
    "\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(f\"[INFO] Couldn't find torchinfo...installing it.\")\n",
    "    !pip3 install -q torchinfo\n",
    "    from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b129e-993e-4de6-a332-d6874700926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device agnostic code \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200349a-2ce7-4cb4-887e-09e079b9839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "# Download pizza, steak, sushi images from github\n",
    "image_path = download_data.download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                                         destination=\"pizza_steak_sushi\")\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fedac62-7005-43ef-8aae-a08681795b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images \n",
    "train_directory = image_path / \"train\"\n",
    "test_directory = image_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ec975-5e96-4731-a69c-91e13c2d93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Datasets and DataLoaders:\n",
    "\"\"\" \n",
    "  First we need to need to create transforms to prepare for our images.\n",
    "  This is where our reference to the paper will come in.\n",
    "  In Table 3, the training resolution is mentioned as being 224(height=224, width=224)\n",
    "\n",
    " # The Vit paper also states the use of batch of 4096 ,\n",
    "   which is quite big for our data(downlaoded) and because hardware may not be able to handle this batch size,\n",
    "   we are going to use batch size of 32\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a2e7f-8ce0-4ccd-93b9-da3222c9daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare transforms for images :\n",
    "# Create image szie (form ViT paper)\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Create transform pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "                                        transforms.Resize(size=(IMAGE_SIZE,IMAGE_SIZE)),\n",
    "                                        transforms.ToTensor()\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b12938-a6c3-4c8e-868c-69e8cad0e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn images into DataLoaders\n",
    "import os\n",
    "# Set the batch size(convinient)\n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloaders, test_dataloaders , class_names = data_setup.create_dataloaders(train_dir=train_directory,\n",
    "                                                                                  test_dir=test_directory,\n",
    "                                                                                  train_transform=manual_transforms,\n",
    "                                                                                  test_transform=manual_transforms,\n",
    "                                                                                  batch_size=BATCH_SIZE,\n",
    "                                                                                  num_workers=os.cpu_count()\n",
    "                                                                                 )\n",
    "\n",
    "                                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156718a9-6924-4e50-8dde-d700b2f216c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a single image :\n",
    "# Get the batch of images \n",
    "image_batch, label_batch = next(iter(train_dataloaders))\n",
    "# Get a single image\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "# View the sample image \n",
    "image.shape, label\n",
    "\n",
    "# plot the image \n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc52d8-21c5-42d8-a640-a85ea0034df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATA INTO PATCHES AND CREATING THE CLASS,POSITION AND PATCH EMBEDDING\n",
    "\"\"\"\n",
    "We can represent the data in a good, learnable way(as embeddings are learnable representations), chances are, a learning algorithm will be able to perform welll on them.\n",
    "\n",
    "Starting with patch embedding --This means that we will turn input images in a sequence of patches and then embedd those patches.\n",
    "Embedding is a learnable representation and often is a vector.\n",
    "\n",
    "From the ViT paper :\n",
    "  The standard transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the images x with dimension HxDxC into a sequence of flattened 2D patches xp where (H,W)is the resolution\n",
    "  of the original image, C is the number of channels, (P,P) is the resolution of each image patch, and N = HW/P*P is the resulting number of patches, which also serves as the effective input sequence length for the\n",
    "  transformer.\n",
    "  The transformer uses constant latent vector size D through all its layers, so we can flatten the patches and map to D dimensions with a trainable linear projection.The output of this projection is patch embeddings.\n",
    "\n",
    "So,\n",
    "  D is the size of the patch embeddings, different values for D for various sized ViT models can be found in paper Table 1.\n",
    "  The image starts as 2D with size H x W x C.\n",
    "      * (H, W) is the resolution of the original image.\n",
    "      * C is the number of color channels.\n",
    "  The image gets converted to a sequence of flattened 2D patches with size N x (P*P . C)\n",
    "      * (P,P) is the resolution of each image patch(patch size)\n",
    "      * N = HW/P*P is the resulting number of patches, which also serve as the input sequence length for the transformer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e997e69c-9dc5-4f17-b2fc-095163a0815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating patch embedding input and output shapes by hand \n",
    "\"Let's create variables to mimic each of the terms\"\n",
    "\" We'll use a patch size(P) of 16 since it's the best performing version of ViT-Base uses\"\n",
    "\n",
    "\n",
    "# Create example values \n",
    "height = 224      # H\n",
    "width = 224       # W\n",
    "color_channels =3 # C\n",
    "patch_size = 16   # P\n",
    "\n",
    "# Calculate N (number of patches)\n",
    "number_of_patches = int((height*width)/patch_size**2)\n",
    "print(f\"Number of patches(N) with image height (H={height}), width (W={width}) and patch size (P={patch_size})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1529129-fe41-4d7b-9b04-38b4b3bdc4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replicate the input and output shapes of the patch embedding layer\"\n",
    "\"\"\"\n",
    "Input : the image starts as 2D with size H x W x C.\n",
    "Output : The image gets converted to a sequence of flattened 2D patches with size N x (P**2 . C)\n",
    "\"\"\"\n",
    "\n",
    "# Input shape(size of a single image)\n",
    "embedding_layer_input_shape = (height, width, color_channels)\n",
    "\n",
    "# Output shape \n",
    "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
    "\n",
    "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
    "print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")    \n",
    "\n",
    "\"Ideal Input and Output shape for patch embedding layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6c9ee-15ff-480a-baf8-4204e8569669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do we create the patch embedding layer?\n",
    "\"Let's turn a single image into patches\"\n",
    "\n",
    "# View a single image \n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67af8a-06df-414c-82ee-cebe8d74eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top row of the image \n",
    "image_permuted = image.permute(1,2,0)  # H*W*C    , compatible with matplotlib\n",
    "\n",
    "# Index the plot to top row of patched pixels\n",
    "patch_size = 16 # for the totality of cell code \n",
    "plt.figure(figsize=(patch_size,patch_size))\n",
    "plt.imshow(image_permuted[:patch_size, :, :]);     # we want the row upto 16 pixels , all the column pixels and all the color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711867f-d9d4-4378-864b-39a8a0663a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the top row into patches :\n",
    "\n",
    "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_patches = img_size/patch_size\n",
    "assert img_size % patch_size == 0, \"Image must be divisible by patch size\"\n",
    "print(f\"Number of patches per row: {num_patches}\\nPatch Size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "# Create a series of subplots \n",
    "fig, axs = plt.subplots(nrows=1,\n",
    "                       ncols=img_size//patch_size,\n",
    "                       figsize=(num_patches, num_patches),\n",
    "                       sharex=True,\n",
    "                       sharey=True)\n",
    "\n",
    "# Iterate through number of patches in the top row\n",
    "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
    "    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]);\n",
    "    axs[i].set_xlabel(i+1)  # Set the label\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c04418-cf42-4a32-9a20-9995959bf34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do these patches for the whole image \n",
    "\n",
    "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_patches = img_size // patch_size\n",
    "assert img_size % patch_size == 0, \"Image must be divisible by patch size\"\n",
    "print(f\"Number of patches per row: {num_patches}\\\n",
    "        \\nNumber of patches per column: {num_patches}\\\n",
    "        \\nTotal patches: {num_patches*num_patches}\\\n",
    "        \\nPatch size: {patch_size} pixels X {patch_size} pixels\")\n",
    "\n",
    "# Create a series of subplots \n",
    "fig, axs = plt.subplots(nrows=img_size // patch_size,\n",
    "                        ncols=img_size // patch_size,\n",
    "                        figsize=(num_patches,num_patches),\n",
    "                        sharex=True,\n",
    "                        sharey=True)\n",
    "\n",
    "# Loop through height and width of image \n",
    "for i, patch_height in enumerate(range(0, img_size, patch_size)):    # iterate through height\n",
    "    for j, patch_width in enumerate(range(0, img_size, patch_size)):  # iterate through width\n",
    "\n",
    "        # plot the permuted image patch\n",
    "        axs[i,j].imshow(image_permuted[patch_height:patch_height+patch_size,   # iterate through height\n",
    "                                       patch_width:patch_width+patch_size,     # iterate through width \n",
    "                                       :])\n",
    "\n",
    "        # Setup the label information, remove the ticks for clarity and set labels to outside \n",
    "        axs[i,j].set_ylabel(i+1,\n",
    "                            rotation=\"horizontal\",\n",
    "                            horizontalalignment=\"right\",\n",
    "                            verticalalignment=\"center\")\n",
    "        axs[i,j].set_xlabel(j+1)\n",
    "        axs[i,j].set_xticks([])\n",
    "        axs[i,j].set_yticks([])\n",
    "        axs[i,j].label_outer()\n",
    "\n",
    "# Set a super title \n",
    "fig.suptitle(f\"{class_names[label]} -> Patchified, fontsize=16\")\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5420ad-8d1a-40f8-905e-b59d54a7da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now how do we turn each of these patches into embedding and convert them into a sequence?\n",
    "\"\"\"\n",
    "We have seen what an image looks like when it gets turned into patches, now let's start moving towards replicating the patch embedding layers with PyTorch.\n",
    "The above operation of creating patches of an image is very similar to the convolutional operation in a CNN.\n",
    "\n",
    "Referencing the ViT paper, in section 3.1 it is mentioned that the patch embedding is achievable with a convolutional neural netwrok(CNN).\n",
    "\n",
    "\"Hybrid architrecture. As an alternate to raw image patches, the input sequence can be formed from feature maps of an CNN. In this hybrid model, the patch embeddding projection E(Eq 1)\n",
    "is applied to patches extracted from a CNN feature map.As a special case, the patches can have spatial size 1x1,which means that the input sequenceis obtained by simply flattening \n",
    "the spatial dimensions of the feature map and projecting to the transformer dimension.The classification input embedding and position emnedding are added as described above.\"\n",
    "\n",
    "The \"feature map\" referred to are the weights/activations produced by the convolutional layer passing over an given image.\n",
    "By setting the \"kernel_size\" and \"stride\" parameters of a torch.nn.Conv2d() layer equal to \"patch_size\", we can effectively get a layer\n",
    "that splits our image into patches and creates a learnable embedding of each patch.\n",
    "\n",
    "# For our image size of 224 and patch szie 16:\n",
    "* Input : (2D image) 224,224,3 -> (height, width, color_channels)\n",
    "* output: (flattened 2D patches) (196,768) -> (number of patches, embedding dimension)\n",
    "\n",
    "# We can recreate these with :\n",
    "* torch.nn.Conv2d(): for turning image into patches of CNN feature maps.\n",
    "* torch.nn.Flatten(): for flattening the spatial dimension of the feature map.\n",
    "\n",
    "# kernel_size = patch_size -> each convolutional kernel will be of size (patch_size, patch_size) \n",
    "# stride = patch_size -> each step of the convolutional kernel will be patch_size\n",
    "# Set in_channels=3 for 3 color_channels and out_channels=768(the same as D value in Table 1 of ViT-Base(this is the embedding dimension, learnable vector of sixe 768))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3199f-80d5-43a9-9ee0-86f83ce76880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Set the patch size \n",
    "patch_size = 16\n",
    "\n",
    "# Create the Conv2D layer with hyperparameters from the ViT paper\n",
    "conv2d = nn.Conv2d(in_channels=3,\n",
    "                   out_channels=768,\n",
    "                   kernel_size=patch_size,\n",
    "                   stride=patch_size,\n",
    "                   padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e296e-5a7c-4708-8971-f622b1576c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the image through the convolutional layer \n",
    "image_conv = conv2d(image.unsqueeze(0))   # Add a single dimension(HxWxC)-> (BxHxWxC)\n",
    "print(image_conv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab2a43-28df-47fe-8f20-8ccaa6d647b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot five random convoltional feature maps :\n",
    "\n",
    "import random\n",
    "random_idx = random.sample(range(0,768), k=5)  # pick 5 numbers between 0 and 768\n",
    "print(f\"Showing random convolutional feature maps from indexes: {random_idx}\")\n",
    "\n",
    "# Create Plot:\n",
    "fig ,axs = plt.subplots(nrows=1, ncols=5, figsize=(12,12))\n",
    "\n",
    "# Plot random image feature maps\n",
    "for i, index in enumerate(random_idx):\n",
    "    image_map = image_conv[:, index, :, :]   # index on the output tensor of the convolutional layer \n",
    "    axs[i].imshow(image_map.squeeze().detach().numpy())\n",
    "    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    \n",
    "# Get a single feature map in tensor form \n",
    "single_feature_map = image_conv[:, 0, :, :]\n",
    "single_feature_map, single_feature_map.requires_grad\n",
    "#\"\"\"The \"grad_fn\" output of the single_feature_map and the \"requires_grad=True\" attribute means PyTorch is tracking the gradients of this feature map \n",
    "# and it will be updated by gradient descent during training\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efd5b7-12ca-4ace-9937-acc8417df049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the patch embedding with torch.nn.Flatten()\n",
    "\n",
    "# Desired output(1D sequence of flattened 2D patches): (196,768)-> (number_of_patches, embedding dimensions)  -> Nx(P**2*C)\n",
    "# We don't want to flatten the whole tensor but only want to flatten the \"spatial dimensions of the feature map\".(in this case are the feature_map_height and feature_map_width dimeansions)\n",
    "\n",
    "# Create flatten layer\n",
    "flatten = torch.nn.Flatten(start_dim=2,   # flatten feature_map_height (dimension=2)\n",
    "                           end_dim=3)     # flatten feature_map_width (dimension=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4305178-224d-48c5-84b7-483bd1999715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all together :\n",
    "#1. View a single image \n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)\n",
    "print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "# 2. Turn image into feature map\n",
    "image_conv = conv2d(image.unsqueeze(0))\n",
    "print(f\"Image feature map shape: {image_conv.shape}\")\n",
    "\n",
    "#3. Flatten the feature map\n",
    "image_flattened = flatten(image_conv)\n",
    "print(f\"Flattened image feature map shape: {image_flattened.shape}\")\n",
    "\n",
    "# the desired output is (196, 768) and the current output is (1,768,196) so we need to reshape our output \n",
    "\n",
    "image_flattened_reshaped = image_flattened.permute(0,2,1)   # (batch size, number of patches, embedding dimensions)\n",
    "print(f\"Patch embedding sequence shape: {image_flattened_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a69358-9d27-4687-8ad6-37d731ceb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatig the patch embedding layer into a PyTorch module\n",
    "\" Subclassing nn.Module and using above information\"\n",
    "\n",
    "#1. Create a class \n",
    "class patchembedding(nn.Module):\n",
    "    \"\"\"Turns a 2D image into a 1D sequence of learnable embedding vector.\n",
    "\n",
    "    Args:\n",
    "    in_channels(int) : Number of color channels for the input images. Default to 3.\n",
    "    patch_size(int) : Size of patches to convert input image into. Defaults to 16.\n",
    "    embedding_dim(int) : Size of embedding to turn image into. Defaults to 768.\n",
    "\"\"\"\n",
    "    # 2. Initialize the class with appropriate variables \n",
    "    def __init__(self, \n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=16,\n",
    "                 embedding_dim:int=768):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create a layer to turn an image into patches(feature map)\n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "        #4. Create a layer to flatten teh patch feature map into a single dimension \n",
    "        self.flatten = nn.Flatten(start_dim=2,\n",
    "                                  end_dim=3)\n",
    "\n",
    "\n",
    "    # Define the forward method\n",
    "    def forward(self, x):\n",
    "        image_resolution = x.shape[-1]\n",
    "        # Create assertion to check that inputs are the correct shape\n",
    "        assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch_size: {patch_size}\"\n",
    "\n",
    "        # perform the forward pass\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched)\n",
    "\n",
    "        return x_flattened.permute(0,2,1)  # Adjust so that the embedding is on the final dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a1abf-50ac-48e1-bf96-eefd09931fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds.set_seeds()\n",
    "\n",
    "patchify = patchembedding(in_channels=3,\n",
    "                          patch_size=16,\n",
    "                          embedding_dim=768)\n",
    "\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0))\n",
    "print(f\"Output image shape: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5acc61-ae0d-4329-b14e-a0852b31dd0a",
   "metadata": {},
   "source": [
    "# Get summary \n",
    "random_input = (1,3,224,224)\n",
    "summary(patchembedding(),\n",
    "        input_size=random_input,\n",
    "        col_names =[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a4ea12-dd95-4154-a4e4-865b1aa1f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE CLASS TOKEN EMBEDDING \n",
    "\"\"\"\n",
    "X(CLASS) FOR Eq 1 FORM THE ViT PAPER.\n",
    "FROM THE SECOND PARAGRAPH OF SECTION 3.1 FROM THE ViT PAPER, WE SEE:\n",
    "   \" Similar to BERT's [class] token , we prepend a learnable embedding to the sequence of embedded patches (z=x(class)), whose state at the output of the transformer encoder serves as the image representation y(Eq 4).\"\n",
    "\n",
    "So we need to \"prepend a learnable embedding to the sequence of embedded patches\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c000f-8538-4f7c-82ba-620278bb190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a learnable embedding in the shape of the embedding_dimensions(D) and then add it to the number_of_patches dimension.\n",
    "\n",
    "\"\"\"\n",
    "pseudocode:\n",
    " patch_embedding = [image_patch_1, image_patch_2, ...]\n",
    " class_token = learnable_embedding\n",
    " patch_embedding_with_class_token = torch.cat((class_token, patch_embedding)),dim=1)\n",
    "\n",
    "The torch.cat() happens on dim=1 (the number_of_patches dimension)\n",
    "To create a learnable embedding :\n",
    " we'll get the batch szie and embedding dimension shape and then we'll create a torch.ones() tensor in the shape [batch_size, 1, embedding_dimension]\n",
    "\n",
    "We'll make teh tensor learnable by passing it to nn.Parameter() with requires_grad=true.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345f996-66a5-4256-ac37-467dd6ec75b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the batch size and embedding dimension\n",
    "batch_size = patch_embedded_image.shape[0]\n",
    "embedding_dimension = patch_embedded_image.shape[2] \n",
    "# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension\n",
    "class_token = nn.Parameter(torch.randn(batch_size, 1, embedding_dimension),     # [ batch_size, number_of_token, embedding_dimension]\n",
    "                           requires_grad=True)      # to make sure the embedding is learnable\n",
    "\n",
    "# show the first 10 examples of the class_token \n",
    "print(class_token[:, :, :10])\n",
    "\n",
    "# print the class token shape\n",
    "print(f\"Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embeddding dimension]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5dce5-e20c-4e3a-a727-7ff5c5211276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now prepend the class_token to sequence of image patches \n",
    "\n",
    "# Add the class token embedding to the front of the patch embedding \n",
    "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),dim=1)  # Concat on first dimension\n",
    "\n",
    "# print it out \n",
    "print(patch_embedded_image_with_class_embedding)\n",
    "print(f\"Sequence of patch embedding with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -> [ batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235c75a-a7ce-4640-90fb-a8f6ec85261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE POSITION EMBEDDING\n",
    "\n",
    "\"\"\"\n",
    "From the Vit paper section 3.1:\n",
    "    \" Position embeddings are added to the patch embeddings to retain positional information. We use standard leanable 1D position embedding, since we have not observed significant performanece gains using more advanced 2D\n",
    "     aware positional embeddings. The resulting sequence of embedding vectors serves as input to the encoder\"\n",
    "\n",
    "By 'retain positional information', the authors mean they want teh architecture to know what \"order\" the patches come in.\n",
    "This positional information is important whwn considering what's in an image(without positional information and a flattened sequence couls be seen as having no order and thus no patch relates to any other patch).\n",
    "\"\"\"\n",
    "\n",
    "# From Eq 1 we have :\n",
    "\"\"\"\n",
    "E(pos) should have shape (N+1)*D\n",
    "where,\n",
    "* N= HW/P**2  is the resulting number of patches, which also serves as effective input sequence lenght for the transformer.\n",
    "* D is the size of the patch embeddings, differrent value of D can be found in Table 1(embedding dimension)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e3f9c-6473-4d38-8f4b-128e6fa434c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of patches :\n",
    "num_of_patches = int((height*width) / patch_size**2)\n",
    "\n",
    "# Embedding Dimension:\n",
    "embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n",
    "\n",
    "# Create a leanable 1D positional embedding\n",
    "positional_embedding = nn.Parameter(torch.randn(1,\n",
    "                                               num_of_patches+1,\n",
    "                                               embedding_dimension),\n",
    "                                    requires_grad = True)\n",
    "print(positional_embedding[:, :10, :10])\n",
    "print(f\"Positional embedding shape: {positional_embedding.shape} -> [batch_size, number of patches, embedding dimension]\")\n",
    "\n",
    "# Add the positional embedding to the patch and class embeddings \n",
    "\n",
    "patch_and_position_embedding = patch_embedded_image_with_class_embedding + positional_embedding \n",
    "print(patch_and_position_embedding)\n",
    "print(f\" Patch embedding, class token prepend and positional embeddings added shape: {patch_and_position_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1c333-5b2b-409c-aec4-3b800fc0624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together :\n",
    "\n",
    "set_seeds.set_seeds()\n",
    "# 1. Set patch size\n",
    "patch_size = 16\n",
    "\n",
    "# 2. print the shape of original image tensor and get the image dimensions:\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[1], image.shape[2]\n",
    "\n",
    "# 3. Get image tensor and add batch dimension\n",
    "x = image.unsqueeze(0)\n",
    "print(f\"Image tensor with batch dimension shape: {x.shape}\")\n",
    "\n",
    "# 4. Create patch embedding layer:\n",
    "patch_embedded_layer = patchembedding(in_channels=3,\n",
    "                                      patch_size=patch_size,\n",
    "                                      embedding_dim = 768)\n",
    "# 5. PAss image through patch embedding layer \n",
    "patch_embedding = patch_embedded_layer(x)\n",
    "print(f\"Patch embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "# 6. Create class token embedding:\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), requires_grad=True)\n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "# 7. Prepend class token embedding to patch embedding\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\") \n",
    "\n",
    "# 8. Create positional embeddings :\n",
    "number_of_patches = int((height*width) / patch_size**2)\n",
    "positional_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension), requires_grad=True)\n",
    "print(f\"Position embedding shape: {positional_embedding.shape}\")\n",
    "\n",
    "# 9. Add position embedding to patch embedding with class token \n",
    "patch_and_position_embedding = patch_embedding_class_token + positional_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230e4e4-49b0-4157-9b02-bb3dca0a51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EQUATION 2 : MULTI-HEAD ATTENTION(MSA)\n",
    "\"\"\"\n",
    "The transformer encoder section is divided into two parts: the first being equation 2 and the second being equation 3\n",
    "\n",
    "From equation 2 states:\n",
    " \"A multi-head attention(MSA) layer wrapped in a LayerNorm(LN) layer with a residual connection(the input to the layers gets added to the output of the layer)\"\n",
    "Equation 2 refers to MSA block.\n",
    "\n",
    "We can replicate these layers by using :\n",
    "* Multi-Head Self Attention(MSA) - torch.nn.MultiheadAttention()\n",
    "* Norm (LN or LayerNorm) - torch.nn.LayerNorm()\n",
    "* Residual connection - add the input to output\n",
    "\"\"\"\n",
    "\n",
    "# Replicating Equation 2 with Pytorch layers :\n",
    "\"\"\"\n",
    "1. Create a  class called MultiheatSelfAttentionBlock that inherits from nn.Module.\n",
    "2. Initialize the class with hyperparameters from Table 1 of the ViT paper from the ViT-Base model.\n",
    "3. Create a layer normalization(LN) layer with torch.nn.LayerNorm() with the normalized_shape parameter the same as our embedding dimension(D form Table 1)\n",
    "4. Create a multi-head attention(MSA) layer with the appropraite embed_dim, num_heads, dropout, and batch_first parameter.\n",
    "5. Create a forward() method for our class passing the inputs throught the LN layer and the MSA layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017d972-13c3-4761-98b3-81fbf4192133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create a class\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    #2. Initialize the class with hyperparameters:\n",
    "    def __init__(self, \n",
    "                 embedding_dim: int=768,\n",
    "                 num_heads: int=12,    # Heads from Table 1 of ViT paper ViT-Base\n",
    "                 attn_dropout: float=0): # No dropout used in paper for MSA block\n",
    "        super().__init__()\n",
    "\n",
    "        #3. Create a Norm Layer(LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        #4. Create a multi head self attention (MSA) layer \n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True)    # Does our batch dimension come first?\n",
    "\n",
    "    # 5. Create a forward function\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x,  # query embeddings\n",
    "                                          key=x,    # key embeddings \n",
    "                                          value=x,  # value embeddings \n",
    "                                          need_weights=False)   # do we need weights or just the layer output?\n",
    "        return attn_output\n",
    "\n",
    "# Create an instance of MSA block\n",
    "\n",
    "multihead_self_attentionblock = MultiheadSelfAttentionBlock(embedding_dim=768,  # from Table 1\n",
    "                                                           num_heads=12)       # from Table 1\n",
    "\n",
    "# Pass patch and position embedding through MSA block\n",
    "patched_image_msa_block = multihead_self_attentionblock(patch_and_position_embedding)\n",
    "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
    "print(f\"Output shape of MSA block: {patched_image_msa_block.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290f1ee-82ca-42a6-ace2-e1b2940e84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EQUATION 3 : MULTILAYER PERCEPTRON (MLP)\n",
    "\"\"\" It is referred to as the MLP block of the transformer encoder:\n",
    "The term MLP is quite broad as it can refer to almost any combination of multiple layers:\n",
    "Generally it follows the pattern:\n",
    "linear layer -> Non-linear layer -> linear layer -> Non-linear layer\n",
    "IN case of ViT paper the MLP structure is defined in section 3.1 as :\n",
    "\" The MLP contains 2 linear layers with a GELU non-linearity.\"\n",
    "GELU non linearity: refers to Gussian Error Linear Units non activation function.\n",
    "Also in Appendix B.1 it says :\n",
    "\"Table 3 summarizes out training setups for our different models...Droupout, when used, is applied after every dense layer except for the qkv-projrctioins and directly after adding positional-to patch embeddings\"\n",
    "This means that every linear layer(or dense layer) in MLP block has a dropout layer, value of which can be found in Table 3 of ViT paper(ViT-Base, dropout=0.1)\n",
    "\n",
    "\n",
    "Knowing this the structure of our MLP is :\n",
    "layer norm -> linear layer -> non-linear layer -> dropout -. linear layer -> dropout\n",
    "\n",
    "With the hyper parameters values for the linear layer from Table 1(MLP size is the number of hidden units between the linear layers and the hidden size D is the output of the MLP block).\n",
    "\"\"\"\n",
    "\n",
    "# Replicating Equation 3 with Pytorch layers :\n",
    "\"\"\"\n",
    "1. Create a  class called MultilayerPerceptronBlock that inherits from nn.Module.\n",
    "2. Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper from the ViT-Base model.\n",
    "3. Create a layer normalization(LN) layer with torch.nn.LayerNorm() with the normalized_shape parameter the same as our embedding dimension(D form Table 1)\n",
    "4. Create a sequential series of MLP layer(s) using torch.nn.Linear(),torch.nn.Dropout() and torch.nn.GELU() with appropriate hyperparameter values from Table 1 and Table 3.\n",
    "5. Create a forward() method for our class passing the inputs throught the LN layer and the MLP layer(s).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa666ca-a7ea-4182-ab9a-c4579b8c43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create a class\n",
    "class MultilayerPerceptronBlock(nn.Module):\n",
    "    #2. Initialize the hyperparameters \n",
    "    def __init__(self, \n",
    "                 embedding_dim:int=768,   # Hidden size D from Table 1 from ViT-Base\n",
    "                 mlp_size:int=3072,       # MLP size form Table 1 \n",
    "                 dropout: float=0.1):     # dropout from Table 3\n",
    "        super().__init__()\n",
    "        \n",
    "        #3. Create the Norm Layer\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        #4. Create MLP layer(s)\n",
    "        self.mlp = nn.Sequential(\n",
    "                                nn.Linear(in_features=embedding_dim,\n",
    "                                          out_features=mlp_size),\n",
    "                                nn.GELU(),\n",
    "                                nn.Dropout(p=dropout),\n",
    "                                nn.Linear(in_features=mlp_size,\n",
    "                                          out_features=embedding_dim),\n",
    "                                nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    #5. Create a forward method \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create instance of MLP block\n",
    "mlp_block = MultilayerPerceptronBlock(embedding_dim=768,\n",
    "                                      mlp_size=3072,\n",
    "                                      dropout=0.1)\n",
    "\n",
    "patched_image_mlp_block = mlp_block(patched_image_msa_block)\n",
    "print(f\"Input shape : {patched_image_msa_block.shape}\")\n",
    "print(f\"Output shape : {patched_image_mlp_block.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70cbd36-6ea6-4fbd-bb7f-f94a261d954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE TRANSFORMER ENCODER OF ViT-Base BY COMBINING EQUATION 2 AND 3 AND THE CODE BLOCKS FROM ABOVE \n",
    "\"\"\"\n",
    "In deep learning, 'Encoder' or 'Auto encoder' generally refers to a stack of layers that encodes an input(turns it some form of numerical representation)\n",
    "Transformer Encoder will encode our patched image embedding into a learned representation using a series of alternating layers of MSA blocks and MLP blocks , as per section 3.1 of ViT paper:\n",
    "  ' The transformer encoder consists of alternating layers of multihead selfattention(MSA) and MLP blocks. LayerNorm(LN) is applied before every block,a nd residual connections after every block'\n",
    "\n",
    "Residual Connections : also called skip connections, are achieved by adding a layer(s) input to its subsequent output.\n",
    "In case of ViT architecture, residual connections means the input of the MSA block is added beck to the output of the MSA block before if passes to the MLP block.\n",
    "\n",
    "Pseudocode :\n",
    " x_input -> MSA block -> [MSA block_output + x_input] -> MLP block -> [MLP block_output + MSA block_output + X_input] -> ...\n",
    "\n",
    "MAIN IDEA : of residual connections is that they prevent weight values and gradient updates from getting too small and thus allow deeper networks and in turn allow deeper representations to be learned.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## Creating a transformer encoder by combining our custom made layers :\n",
    "\"\"\"\n",
    "1. Create a  class called TransformerEncoderBlock that inherits from nn.Module.\n",
    "2. Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper from the ViT-Base model.\n",
    "3. Instantiate MSA block for equation 2 using MultiheadSelfAttentionBlock from above with appropriate parameters.\n",
    "4. Instantiate MLP block for equation 3 using our MultilayerPerceptronBlock from above with appropriate parameter.\n",
    "5. Create a forward() method for our TranformerEncoderBlock class.\n",
    "6. Create a residual connection for MSA block.\n",
    "7. Create a residual connection for MLP block.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77906485-2d0a-4fd3-8351-f1930a16a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    #2. Initialize the class with hyperparameters\n",
    "    def __init__(self, \n",
    "                 embedding_dim:int=768,\n",
    "                 num_heads:int=12,\n",
    "                 mlp_size:int=3072,\n",
    "                 mlp_dropout:float=0.1,\n",
    "                 attn_dropout:float=0):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create a MSA block:\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        #4. Create a MLP block:\n",
    "        self.mlp_block = MultilayerPerceptronBlock(embedding_dim=embedding_dim,\n",
    "                                              mlp_size=mlp_size,\n",
    "                                              dropout=mlp_dropout)\n",
    "    #5. Create forward method:\n",
    "    def forward(self, x):\n",
    "        #6 Create residual connections for MSA block\n",
    "        x = self.msa_block(x) + x\n",
    "        #7. Create residual connections for mlp block\n",
    "        x = self.mlp_block(x) + x \n",
    "\n",
    "        return x \n",
    "\n",
    "# Create an instance of TransformerEncoderBlock\n",
    "transformer_encoder = TransformerEncoderBlock()\n",
    "\n",
    "summary(model=transformer_encoder,\n",
    "        input_size=(1,197,768),\n",
    "        col_names = [\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e1bb8c-77c7-4e23-ad0d-a23af491d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a  Transformer Encoder with PyTorch's Transformer Layers\n",
    "\" we can recreate the TranformerencoderBlock using torch.nn.TransformerEncoderLayer() and setting some hyperparameter\"\n",
    "\n",
    "torch_transformer_encoder = torch.nn.TransformerEncoderLayer(d_model=768,   # Hidden size D from Table 1 For ViT-Base\n",
    "                                                             nhead =12,     # heads from Table 1\n",
    "                                                             dim_feedforward=3072,   # MLP size from Table 1\n",
    "                                                             dropout=0.1,   # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "                                                             activation=\"gelu\",    # GELU non linear activation\n",
    "                                                             batch_first=True,\n",
    "                                                             norm_first=True)    # Normalize first or after MSA/MLP layers?\n",
    "\n",
    "summary(model=torch_transformer_encoder,\n",
    "        input_size=(1,197,768),\n",
    "        col_names = [\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36084a9b-c65a-413f-b690-a6d827fd09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUTTING IT ALL TOGETHER TO CREATE ViT ARCHITECTURE:\n",
    "\"\"\" \n",
    "We're going to combine all of the blocks that we have created to replicate the full ViT architecture.\n",
    "From patch and postional embedding to the Transformer Encoder(s) to the MLP head.\n",
    "\n",
    "We'll add Equation 4 into our overall ViT architecture class.\n",
    "All we need is a torch.nn.LayerNorm() and a torch.nn.Linear() layer to convert the 0th index of the Transformer Encoder logit outputs to the target number of classes we have.\n",
    "\n",
    "To create the full architecture we'll also need to stack number of TransformerEncoderBlock s on top of each other, this can be done by\n",
    " passing a list of then to torch.nn.Sequential().\n",
    "\n",
    "Focus on ViT-Base hyperparameters from Table 1 but make the code adaptable to other ViT variants.\n",
    "\"\"\"\n",
    "\n",
    "# Creating the Vit architecture:\n",
    "\"\"\"\n",
    "1. Create a class called ViT that inherits from torch.nn.Module.\n",
    "2. Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base.\n",
    "3. Make sure the image size is divisible by the patch size (the image should be split into even patches)\n",
    "4. Calculate the number of patches using the formula N = HW/P**2, where H is height, W is width and P is is the patch size.\n",
    "5. Create a learnable class embedding token(equation 1) as done above.\n",
    "6. Create a learnable position embedding vector(equation 1) as done above.\n",
    "7. Setup the embedding dropout layer \n",
    "8. Create the patch embedding layer using 'patchembedding' class above.\n",
    "9. Create a series of Transformer Encoder blocks by passing list of TransformerEncoderBlocks created above to torch.nn.Sequential.\n",
    "10. Create teh MLP head(also called teh classifier had of Equation 4) by passing torch.nn.LayerNorm() (LN) layer and a torch.nn.Linear(out_features=num_classes) layer \n",
    "(where the num_classes is the target number of classes) linear layer to torch.nn.Sequential.\n",
    "11. Create the forward method that accepts the input.\n",
    "12. Get the batch size of the input(the first dimension of the shape).\n",
    "13. Create the patch embedding  using layers created in step 8.\n",
    "14. Create the class tokenn embedding using the layer created in step 5 expand it across the number of batches found in step 12 using torch.tensor.expand().\n",
    "15. Concatenate the class token embedding created in step 14 to the first dimension of the patch embedding created in step 13 using torch.cat().\n",
    "16. Add the position embedding created in step 6 to the patch and class token embedding created in step 15.\n",
    "17. Pass the patch and position embedding through the dropout layer created in step 7.\n",
    "18. Pass the patch and position embedding from step 16 through transformer encoder layers created in step 9(Equations 2 & 3).\n",
    "19. Pass index 0 of the output of the stack of transformer Encoder layers from step 18 through the classifier head created in step 10(Equation 4).\n",
    "20. Done.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64195b42-7fee-476a-aae8-a5c649daa79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. create the class\n",
    "class ViT(nn.Module):\n",
    "    \"\"\" Creates a Vision Transformer architecture with ViT-Base hyperparameter by default.\"\"\"\n",
    "    #2. Initialize the class with hyperparameters\n",
    "    def __init__(self, \n",
    "                 img_size:int=224,\n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=16,\n",
    "                 num_transformer_layers:int=12,\n",
    "                 embedding_dim:int=768,\n",
    "                 mlp_size:int=3072,\n",
    "                 num_heads:int=12,\n",
    "                 attn_dropout:float=0,\n",
    "                 mlp_dropout:float=0.1,\n",
    "                 embedding_dropout:float=0.1,    # dropout for patch and position embedding \n",
    "                 num_classes:int=1000):          # Default for ImageNet but can customize this \n",
    "        super().__init__()\n",
    "\n",
    "        # 3.Make the image size divisible by the patch size \n",
    "        assert img_size % patch_size == 0, f\"Image size should be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "\n",
    "        #4. Calculate the number of patches \n",
    "        self.num_patches = (img_size*img_size) // patch_size**2\n",
    "\n",
    "        #5. Create a learnable class embedding token (needs to go at front of sequence of patch embeddings)\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1,1,embedding_dim), requires_grad=True)\n",
    "\n",
    "        #6. Create a learnable position embedding \n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1,self.num_patches+1, embedding_dim), requires_grad=True)\n",
    "\n",
    "        #7. Create embedding dropout value \n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "\n",
    "        #8. Create patch embedding layer \n",
    "        self.patch_embedding = patchembedding(in_channels=in_channels,\n",
    "                                               patch_size=patch_size,\n",
    "                                               embedding_dim=embedding_dim)\n",
    "\n",
    "        #9. create Transformer Encoder block(stacking blocks using nn.Sequential()\n",
    "        # NOTE: '*' means \"ALL\"\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                           num_heads=num_heads,\n",
    "                                                                           mlp_size=mlp_size,\n",
    "                                                                           mlp_dropout=mlp_dropout,\n",
    "                                                                           attn_dropout=attn_dropout) for _ in range(num_transformer_layers)])\n",
    "        #10. Create classifier head \n",
    "        self.classifier = nn.Sequential(\n",
    "                                        nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "                                        nn.Linear(in_features=embedding_dim,\n",
    "                                                  out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    #11. Create forward method\n",
    "    def forward(self, x):\n",
    "        #12. Get batch size \n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        #13. Create class token embedding and expand it to match the batch size \n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1)     # -1 means to infer the dimension\n",
    "\n",
    "        #14. Create patch embedding \n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        #15. Concatenate class token and patch embedding \n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        #16. Add position embedding to patch embedding \n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        #17. Run through embedding dropout \n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        #18. Pass patch. position and class embedding through transformer encoder layers \n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        #19. Put 0 th index logit through classifier\n",
    "        x = self.classifier(x[:, 0])     # run on each sample in a batch at 0 index \n",
    "\n",
    "        return x \n",
    "                                                                           \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0debc-5f7c-4885-938b-2a9a073ed075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example \n",
    "batch_size = 32\n",
    "class_token_single = nn.Parameter(data=torch.randn(1,1,768))\n",
    "class_token_expanded = class_token_single.expand(batch_size, -1, -1)\n",
    "\n",
    "print(class_token_expanded.shape)\n",
    "\n",
    "set_seeds.set_seeds()\n",
    "\n",
    "random_image_tensor = torch.randn(1,3,224,224)\n",
    "\n",
    "# Create an Instance of ViT class\n",
    "vit = ViT(num_classes=len(class_names))\n",
    "\n",
    "vit(random_image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f3baa-6da0-4514-bf1b-430ffb6eef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Visual summary\n",
    "\n",
    "summary(model=vit,\n",
    "        input_size=(32,3,224,224),\n",
    "        col_names = [\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63146b-4cdd-4605-91fc-554fdd4a764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training ViT model\n",
    "\n",
    "# Import engine module form Scripts \n",
    "from Scripts import engine\n",
    "\n",
    "# Setup the optimizer as per ViT model parameters using hyperparamters from the ViT paper \n",
    "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
    "                             lr=3e-3,                     # Base Learning Rate from Table 3 for ViT-* ImageNet-1k\n",
    "                             betas=(0.9,0.999),           # Default values but also mentioned in ViT paper section 4.1 (Training & Finetuning)\n",
    "                             weight_decay=0.3)            # From ViT paper section 4.1 (Training and Fine tuning)\n",
    "\n",
    "# Setup loss function for multiclass classification\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set seeds \n",
    "set_seeds.set_seeds()\n",
    "\n",
    "# Train the model using engine module  and save the training results to a dictionary\n",
    "results = engine.train(model=vit,\n",
    "                       train_dataloader=train_dataloaders,\n",
    "                       test_dataloader=test_dataloaders,\n",
    "                       loss_func=loss_fn,\n",
    "                       optimizer=optimizer,\n",
    "                       epochs=10,\n",
    "                       device=device)\n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee591239-0c84-4c69-981a-cf043193cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts import plot_loss_curves \n",
    "\n",
    "# PLot our ViT model's loss curves \n",
    "plot_loss_curves(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
